# -*- coding: utf-8 -*-
"""APENDICE_TCC_FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10uABhxSY9z8dOdHldmgiLXN8KX3IrtS8
"""

import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SpatialDropout2D, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.regularizers import l1, l2, l1_l2
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model
from tensorflow.keras.models import model_from_json
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from google.colab import drive
drive.mount('/content/drive')

# ---------------------------------------------------------------------------------
# Classes: ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
# ---------------------------------------------------------------------------------
data = pd.read_csv("/content/drive/My Drive/fer2013.csv")
data.tail()


#
# Gera array de faces onde cada face tem tamanho 48 x 48
#
pixels = data["pixels"].tolist()
largura, altura = 48, 48
faces = []
amostras = 0
for pixel_sequence in pixels:
    face = [int(pixel) for pixel in pixel_sequence.split(" ")]
    face = np.asarray(face, dtype='uint8').reshape(largura, altura)
    faces.append(face)

print("Número total de imagens no dataset: ", str(len(faces)))


#
# Converte para um array numpy de 4 posicoes, no formato
# esperado pelo TensorFlow
#
faces = np.asarray(faces)
faces = np.expand_dims(faces, -1)


#
# Normaliza o array de faces, originalmente composto por valores
# inteiros de zero a 255, que é o valor do pixel.
# 
# O TensorFlow espera valores do tipo float entre zero e 1
#
def normalizar(x):
    x = x.astype("float32")
    x = x / 255.0
    return x

faces = normalizar(faces)

emocoes = pd.get_dummies(data["emotion"]).values
print(emocoes[0])

#
# Divide o dataset em dados de treino, validacao e teste
#
X_train, X_test, y_train, y_test = train_test_split(
    faces, emocoes, test_size=0.1, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.1, random_state=41
)

print("Número de imagens no conjunto de treinamento:", len(X_train))
print("Número de imagens no conjunto de teste:", len(X_test))
print("Número de imagens no conjunto de validação:", len(X_val))

np.save("mod_xtest", X_test)
np.save("mod_ytest", y_test)

#
# Constroi o generator que faz modificacoes nas imagens de entrada na rede
#
data_generator = ImageDataGenerator(
                        featurewise_center=False,
                        featurewise_std_normalization=False,
                        rotation_range=10,
                        width_shift_range=0.1,
                        height_shift_range=0.1,
                        zoom_range=.1,
                        horizontal_flip=True)

num_features = 64
num_labels = 7
batch_size = 64
epochs = 100
width, height = 48, 48



#
# Constroi o modelo da Rede Neural Convolucional propriamente dita
#
model = Sequential()

model.add(
    Conv2D(
        32,
        kernel_size=(7, 7),
        activation="relu",
        input_shape=(width, height, 1),
        data_format="channels_last",
        kernel_regularizer=l2(0.01),
    )
)
model.add(Conv2D(50, kernel_size=(4, 4), activation="relu", padding="valid"))
model.add(BatchNormalization())
model.add(Conv2D(100, kernel_size=(4, 4), activation="relu", padding="valid"))
model.add(BatchNormalization())
model.add(Conv2D(150, kernel_size=(4, 4), activation="relu", padding="valid"))
model.add(BatchNormalization())
model.add(Conv2D(200, kernel_size=(4, 4), activation="relu", padding="valid"))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(AveragePooling2D(pool_size=(4, 4), strides=(4, 4)))
model.add(Flatten())
model.add(Dense(2 * 2 * 2 * 2 * num_features, activation="relu"))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(2 * 2 * 2 * num_features, activation="relu"))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(2 * 2 * num_features, activation="relu"))
model.add(Dropout(0.75))
model.add(BatchNormalization())
model.add(Dense(num_labels, activation="softmax"))

model.summary()

#
# Compila o modelo
#
model.compile(
    loss="categorical_crossentropy",
    optimizer=Adam(),
    metrics=["accuracy"],
)

#
# Cria callbacks que sao acionados durante o treino da rede neural
# 
lr_reducer = ReduceLROnPlateau(
    monitor="val_loss", factor=0.9, patience=3, verbose=1
)

early_stopper = EarlyStopping(
    monitor="val_loss", min_delta=0, patience=8, verbose=1, mode="auto"
)

arquivo_modelo = "modelo_01_expressoes.h5"
checkpointer = ModelCheckpoint(
    arquivo_modelo, monitor="val_loss", verbose=1, save_best_only=True
)

#
# Salva o modelo em disco em formato humano-legivel
#
model_json = model.to_json()
arquivo_modelo_json = "modelo_01_expressoes.json"
with open(arquivo_modelo_json, "w") as json_file:
    json_file.write(model_json)

#
# Faz o treinamento da rede neural propriamente dita
#
history = model.fit_generator(data_generator.flow(np.array(X_train), np.array(y_train), batch_size),
    epochs=epochs,
    verbose=1,
    validation_data=(np.array(X_val), np.array(y_val)),
    shuffle=True,
    callbacks=[lr_reducer, early_stopper, checkpointer],
)

print(history.history)